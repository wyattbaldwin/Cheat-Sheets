Supervised learning involves training a model on labeled data. Examples include linear regression, logistic regression, and SVM.
Unsupervised learning involves training a model on unlabeled data and finding patterns in the data. Examples include clustering and PCA.
Reinforcement learning involves training a model through interaction and reward/punishment.
Feature selection involves choosing which features to include in a model.
Overfitting occurs when a model is too complex and doesn't generalize well to new data.
Underfitting occurs when a model is too simple and can't capture the underlying patterns in the data.
Cross-validation involves evaluating a model by training it on part of the data and evaluating it on the rest.
Hyperparameter tuning involves adjusting model hyperparameters to improve performance.
Ensemble learning involves combining the predictions of multiple models to improve performance.
The random_state hyperparameter controls randomness in machine learning models and can be set to an integer or None.
When random_state is an integer, the function will produce the same results across different executions. When it is None, the function will produce different results.
Cross-validation can be used to average out the effect of random_state on model performance.
Data preprocessing: the process of preparing the raw data for use in a model. This can include tasks such as cleaning, normalizing, and scaling the data.
Training/validation/test sets: when building a machine learning model, it is common to split the data into three sets: a training set, a validation set, and a test set. The model is trained on the training set, hyperparameters are tuned using the validation set, and the final model is evaluated on the test set.
Loss function: a function that measures the performance of a model on a specific task. The model's goal is to minimize the loss function.
Optimization algorithm: a method used to minimize the loss function and update the model's parameters. Examples include stochastic gradient descent and Adam.
Bias/variance tradeoff: a tradeoff in model complexity, where a model with low bias (i.e., a model that is more complex and can fit the training data well) may have high variance (i.e., poor generalization to new data).
Regularization: a method for reducing overfitting by adding a penalty to the model's complexity. Examples include L1 and L2 regularization.
Transfer learning: a method for using a pre-trained model on a new task, by fine-tuning the model's parameters or using it as a fixed feature extractor.
Deep learning: a subfield of machine learning that involves training artificial neural networks with many layers on large amounts of data.
Activation function: a function that determines the output of a neuron in an artificial neural network. Examples include ReLU and sigmoid.
Convolutional neural network (CNN): a type of neural network specifically designed for image classification tasks, that uses convolutional layers to extract features from the input data.
Recurrent neural network (RNN): a type of neural network that can process sequential data, such as text or time series data.
Autoencoder: a type of neural network used for unsupervised learning, that is trained to reconstruct the input data as closely as possible. Autoencoders can be used for tasks such as dimensionality reduction and anomaly detection.
